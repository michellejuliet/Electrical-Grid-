{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fec5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155b647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'C:\\Users\\user\\Desktop\\MICHELLE\\DATA SCIENCE\\Projects practices\\Electrical Grid Stability Simulated Data.csv'\n",
    "df=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9730d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of           tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0     2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1     9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2     8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3     0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4     3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  2.930406  9.487627  2.376523  6.187797  3.343416 -0.658054 -1.449106   \n",
       "9996  3.392299  1.274827  2.954947  6.894759  4.349512 -1.663661 -0.952437   \n",
       "9997  2.364034  2.842030  8.776391  1.008906  4.299976 -1.380719 -0.943884   \n",
       "9998  9.631511  3.994398  2.757071  7.821347  2.514755 -0.966330 -0.649915   \n",
       "9999  6.530527  6.781790  4.349695  8.673138  3.492807 -1.390285 -1.532193   \n",
       "\n",
       "            p4        g1        g2        g3        g4      stab     stabf  \n",
       "0    -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347  unstable  \n",
       "1    -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957    stable  \n",
       "2    -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471  unstable  \n",
       "3    -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871  unstable  \n",
       "4    -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860  unstable  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995 -1.236256  0.601709  0.779642  0.813512  0.608385  0.023892  unstable  \n",
       "9996 -1.733414  0.502079  0.567242  0.285880  0.366120 -0.025803    stable  \n",
       "9997 -1.975373  0.487838  0.986505  0.149286  0.145984 -0.031810    stable  \n",
       "9998 -0.898510  0.365246  0.587558  0.889118  0.818391  0.037789  unstable  \n",
       "9999 -0.570329  0.073056  0.505441  0.378761  0.942631  0.045263  unstable  \n",
       "\n",
       "[10000 rows x 14 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff1b725a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2405d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tau1', 'tau2', 'tau3', 'tau4', 'p1', 'p2', 'p3', 'p4', 'g1', 'g2',\n",
       "       'g3', 'g4', 'stab', 'stabf'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c3bd2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tau1     0\n",
       "tau2     0\n",
       "tau3     0\n",
       "tau4     0\n",
       "p1       0\n",
       "p2       0\n",
       "p3       0\n",
       "p4       0\n",
       "g1       0\n",
       "g2       0\n",
       "g3       0\n",
       "g4       0\n",
       "stab     0\n",
       "stabf    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c5d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop stab column\n",
    "df.drop(\"stab\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1b4dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tau1', 'tau2', 'tau3', 'tau4', 'p1', 'p2', 'p3', 'p4', 'g1', 'g2',\n",
       "       'g3', 'g4', 'stabf'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "233706fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "# Separate the features and target variable in the training set\n",
    "x_train = train_data.drop(\"stabf\", axis=1)\n",
    "y_train = train_data[\"stabf\"]\n",
    "\n",
    "# Separate the features and target variable in the testing set\n",
    "x_test = test_data.drop(\"stabf\", axis=1)\n",
    "y_test = test_data[\"stabf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bdec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training set\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "# Scale the testing set\n",
    "x_test_scaled = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd2fb7",
   "metadata": {},
   "source": [
    "We'll train four models as specified: Random Forest, Extra Trees, XGBoost, and LightGBM. We'll use the random_state parameter as 1 for training all models and evaluate their performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d51886e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Model training and evaluation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, ExtraTreesClassifier\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# Model training and evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize the classifiers\n",
    "random_forest = RandomForestClassifier(random_state=1)\n",
    "extra_trees = ExtraTreesClassifier(random_state=1)\n",
    "xgboost = XGBClassifier(random_state=1)\n",
    "lightgbm = LGBMClassifier(random_state=1)\n",
    "\n",
    "# Fit the models on the scaled training data\n",
    "random_forest.fit(x_train_scaled, y_train)\n",
    "extra_trees.fit(x_train_scaled, y_train)\n",
    "xgboost.fit(x_train_scaled, y_train)\n",
    "lightgbm.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the scaled testing data\n",
    "rf_predictions = random_forest.predict(x_test_scaled)\n",
    "et_predictions = extra_trees.predict(x_test_scaled)\n",
    "xgb_predictions = xgboost.predict(x_test_scaled)\n",
    "lgbm_predictions = lightgbm.predict(x_test_scaled)\n",
    "\n",
    "# Evaluate the models\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "et_accuracy = accuracy_score(y_test, et_predictions)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_predictions)\n",
    "lgbm_accuracy = accuracy_score(y_test, lgbm_predictions)\n",
    "\n",
    "rf_precision = precision_score(y_test, rf_predictions, average='binary', pos_label='stable')\n",
    "et_precision = precision_score(y_test, et_predictions, average='binary', pos_label='stable')\n",
    "xgb_precision = precision_score(y_test, xgb_predictions, average='binary', pos_label='stable')\n",
    "lgbm_precision = precision_score(y_test, lgbm_predictions, average='binary', pos_label='stable')\n",
    "\n",
    "rf_recall = recall_score(y_test, rf_predictions, average='binary', pos_label='stable')\n",
    "et_recall = recall_score(y_test, et_predictions, average='binary', pos_label='stable')\n",
    "xgb_recall = recall_score(y_test, xgb_predictions, average='binary', pos_label='stable')\n",
    "lgbm_recall = recall_score(y_test, lgbm_predictions, average='binary', pos_label='stable')\n",
    "\n",
    "rf_f1_score = f1_score(y_test, rf_predictions, average='binary', pos_label='stable')\n",
    "et_f1_score = f1_score(y_test, et_predictions, average='binary', pos_label='stable')\n",
    "xgb_f1_score = f1_score(y_test, xgb_predictions, average='binary', pos_label='stable')\n",
    "lgbm_f1_score = f1_score(y_test, lgbm_predictions, average='binary', pos_label='stable')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74db83f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming you have trained a random forest classifier and obtained the predicted labels 'y_pred' for the test set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Also assuming you have the actual labels for the test set in 'y_true'\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate the accuracy\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43my_true\u001b[49m, y_pred)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Print the accuracy with 4 decimal places\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have trained a random forest classifier and obtained the predicted labels 'y_pred' for the test set\n",
    "# Also assuming you have the actual labels for the test set in 'y_true'\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Print the accuracy with 4 decimal places\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eff96d14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming you have trained an XGBoost classifier model and obtained the predicted labels 'y_pred' for the test set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Also assuming you have the actual labels for the test set in 'y_true'\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate the accuracy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have trained an XGBoost classifier model and obtained the predicted labels 'y_pred' for the test set\n",
    "# Also assuming you have the actual labels for the test set in 'y_true'\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Print the accuracy with 4 decimal places\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71b2393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.6-py3-none-win_amd64.whl (70.9 MB)\n",
      "     -------------------------------------- 70.9/70.9 MB 185.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\.virtualbox\\anacondaa\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\.virtualbox\\anacondaa\\lib\\site-packages (from xgboost) (1.10.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.6\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4833d690",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming you have trained an LGBM classifier model and obtained the predicted labels 'y_pred' for the test set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Also assuming you have the actual labels for the test set in 'y_true'\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calculate the accuracy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have trained an LGBM classifier model and obtained the predicted labels 'y_pred' for the test set\n",
    "# Also assuming you have the actual labels for the test set in 'y_true'\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Round the accuracy to 4 decimal places\n",
    "accuracy_rounded = round(accuracy, 4)\n",
    "\n",
    "# Print the accuracy with 4 decimal places\n",
    "print(f\"Accuracy: {accuracy_rounded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d2521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
